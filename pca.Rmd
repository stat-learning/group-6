---
title: "Experiment with PCA on the MNIST dataset"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(MASS)
library(tree)
library(gbm)
library(randomForest)
library(class)
library(e1071)  
```

```{r loading-data}
# This part read idx files and store image data into train$x and 
# test$x in matrix form, store corresponding labels in train$y 
# and test$y in array form 
load_image_file <- function(filename) {
   ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
}

load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
}

train <- load_image_file("data/train-images-idx3-ubyte")
test <- load_image_file("data/t10k-images-idx3-ubyte")

train$y <- load_label_file("data/train-labels-idx1-ubyte")
test$y <- load_label_file("data/t10k-labels-idx1-ubyte")  

```

```{r}
train$x <- train$x/255
test$x <- test$x/255

train_df <- data.frame(train$y, train$x) %>%
  rename(label = train.y)
test_df <- data.frame(test$y, test$x) %>%
  rename(label = test.y)
```


```{r}
# Fit PCA on the training dataset
pca <- prcomp(train_df[, -1])
```

```{r}
# Fit PCA on the test dataset
test_pca <- predict(pca, newdata = test_df)
```

```{r}
# Store the first two coordinates and the label in a data frame
pca_plot <- data.frame(pca_x = pca$x[, "PC1"], pca_y = pca$x[, "PC2"], 
                       label = as.factor(train_df$label))
```

```{r}
# Plot the first two principal components using the true labels as color 
ggplot(pca_plot[1:250,], aes(x = pca_x, y = pca_y, color = label)) + 
	ggtitle("PCA of MNIST sample") + 
	geom_text(aes(label = label)) + 
	theme(legend.position = "none")
```

```{r}
d <- data.frame(PC = 1:784,
                PVE = pca$sdev^2 / sum(pca$sdev^2))
ggplot(d, aes(x = PC, y = PVE)) +
  geom_line() + 
  geom_point() +
  theme_bw(base_size = 18)
```

```{r}
ggplot(d[1:50,], aes(x = PC, y = PVE)) +
  geom_line() + 
  geom_point() +
  theme_bw(base_size = 18)
```

We only need 20 PCs to capture 90% of the variance in our dataset.

```{r}
set.seed(1)
pca <- prcomp(train_df[, -1], rank. = 20)

pca.tr <- data.frame(label = train_df[, 1], pca$x)
pca.tr$label <- as.factor(pca.tr$label)
```

```{r}
pca.tst <- test_pca[, 1:20]  # select the first 20 PCs
pca.tst <- as.data.frame(pca.tst)

pca.tst <- pca.tst %>% mutate(label = test_df$label)
```

## Random Forest
```{r}
set.seed(1)

rf <- randomForest(pca.tr[, -1], pca.tr$label, ntree=500)
rf
```

```{r}
pred.rf <- predict(rf, pca.tst, type = "class")
(conf.rf <- table(pred.rf, pca.tst$label))
```

```{r}
(sum(conf.rf) - sum(diag(conf.rf))) / 
  sum(conf.rf)
```

The misclassification rate is 4.89%. The pair that is most difficult to predict are 4 and 9.

## Classification Tree

```{r}
t <- tree(label ~., data = pca.tr, split = "deviance")
summary(t)
```

```{r}
pred.tree <- predict(t, newdata = pca.tst, type = "class")
(conf.tree <- table(pred.tree, test_df$label))
```

```{r}
(sum(conf.tree) - sum(diag(conf.tree))) / 
  sum(conf.tree)
```

4-9 is still the most difficult pair to predict, followed closely by 5-0, 7-9, 5-3, 5-8.

## Pruning tree

```{r}
t.cv <- cv.tree(t)
plot(t.cv$size, t.cv$dev, type = "b", xlab = "n leaves", ylab = "error")
```

Not a good case for pruning (best n = 13 was already chosen). 

## Bagging tree

```{r}
set.seed(1)

p <- ncol(pca.tr)-1
rf.bag <- randomForest(label ~., data = pca.tr,
                  mtry = p/3, importance = TRUE) 
rf.bag
```

```{r}
varImpPlot(rf.bag)
```

```{r}
pred.bag <- predict(rf.bag, newdata = pca.tst, type = "class")
(conf.bag <- table(pred.bag, pca.tst$label))
```

```{r}
(sum(conf.bag) - sum(diag(conf.bag))) / 
  sum(conf.bag)
```
The misclassification rate is 5.15%.

## Boosting tree

```{r}
set.seed(1)
boost.mnist <- gbm(label~., data = pca.tr, 
                     distribution = "multinomial",
                     n.trees = 50, interaction.depth = 1,
                     shrinkage = 0.1)
summary(boost.mnist)
```

```{r}
pred.boost <- predict(boost.mnist, newdata = pca.tst, n.trees = 50)
pred.boost <- apply(pred.boost, 1, which.max)

(conf.boost <- table(pred.boost, pca.tst$label))
```

```{r}
(sum(conf.boost) - sum(diag(conf.boost))) / 
  sum(conf.boost)
```



## Logistic Regression 

```{r}
# create dummy variables for the digits. 

pca.log.tr <- pca.tr %>% 
  mutate(iszero = as.numeric(label == 0),
         isone = as.numeric(label == 1),
         istwo = as.numeric(label == 2),
         isthree = as.numeric(label == 3),
         isfour = as.numeric(label == 4),
         isfive = as.numeric(label == 5),
         issix = as.numeric(label == 6),
         isseven = as.numeric(label == 7),
         iseight = as.numeric(label == 8),
         isnine  = as.numeric(label == 9))

pca.log.tst <- pca.tst %>% 
  mutate(iszero = as.numeric(label == 0),
         isone = as.numeric(label == 1),
         istwo = as.numeric(label == 2),
         isthree = as.numeric(label == 3),
         isfour = as.numeric(label == 4),
         isfive = as.numeric(label == 5),
         issix = as.numeric(label == 6),
         isseven = as.numeric(label == 7),
         iseight = as.numeric(label == 8),
         isnine  = as.numeric(label == 9))
```

```{r include=FALSE, echo=FALSE}
train.zero <- pca.log.tr %>% dplyr::select(-c(isone, istwo, isthree, isfour, isfive, issix, isseven, iseight, isnine, label))
train.one <- pca.log.tr %>% dplyr::select(-c(iszero, istwo, isthree, isfour, isfive, issix, isseven, iseight, isnine, label))
train.two <- pca.log.tr %>% dplyr::select(-c(iszero, isone, isthree, isfour, isfive, issix, isseven, iseight, isnine, label))
train.three <- pca.log.tr %>% dplyr::select(-c(iszero, isone, istwo, isfour, isfive, issix, isseven, iseight, isnine, label))
train.four <- pca.log.tr %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfive, issix, isseven, iseight, isnine, label))
train.five <- pca.log.tr %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfour, issix, isseven, iseight, isnine, label))
train.six <- pca.log.tr %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfour, isfive, isseven, iseight, isnine, label))
train.seven <- pca.log.tr %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfour, isfive, issix, iseight, isnine, label))
train.eight <- pca.log.tr %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfour, isfive, issix, isseven, isnine, label))
train.nine <- pca.log.tr %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfour, isfive, issix, isseven, iseight, label))
```

```{r include=FALSE, echo=FALSE}
test.zero <- pca.log.tst %>% dplyr::select(-c(isone, istwo, isthree, isfour, isfive, issix, isseven, iseight, isnine, label))
test.one <- pca.log.tst %>% dplyr::select(-c(iszero, istwo, isthree, isfour, isfive, issix, isseven, iseight, isnine, label))
test.two <- pca.log.tst %>% dplyr::select(-c(iszero, isone, isthree, isfour, isfive, issix, isseven, iseight, isnine, label))
test.three <- pca.log.tst %>% dplyr::select(-c(iszero, isone, istwo, isfour, isfive, issix, isseven, iseight, isnine, label))
test.four <- pca.log.tst %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfive, issix, isseven, iseight, isnine, label))
test.five <- pca.log.tst %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfour, issix, isseven, iseight, isnine, label))
test.six <- pca.log.tst %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfour, isfive, isseven, iseight, isnine, label))
test.seven <- pca.log.tst %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfour, isfive, issix, iseight, isnine, label))
test.eight <- pca.log.tst %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfour, isfive, issix, isseven, isnine, label))
test.nine <- pca.log.tst %>% dplyr::select(-c(iszero, isone, istwo, isthree, isfour, isfive, issix, isseven, iseight, label))
```


```{r echo=FALSE}
set.seed(1)

prob.zero <- glm(iszero ~ ., data = train.zero, family = "binomial")
prob.one <- glm(isone ~ ., data = train.one, family = "binomial")
prob.two <- glm(istwo ~ ., data = train.two, family = "binomial")
prob.three <- glm(isthree ~ ., data = train.three, family = "binomial")
prob.four <- glm(isfour ~ ., data = train.four, family = "binomial")
prob.five <- glm(isfive ~ ., data = train.five, family = "binomial")
prob.six <- glm(issix ~ ., data = train.six, family = "binomial")
prob.seven <- glm(isseven ~ ., data = train.seven, family = "binomial")
prob.eight <- glm(iseight ~ ., data = train.eight, family = "binomial")
prob.nine <- glm(isnine ~ ., data = train.nine, family = "binomial")
```


```{r}
ProbabilityOfEachValue <- data.frame(predict(prob.zero, test.zero),
                                     predict(prob.one, test.one),
                                     predict(prob.two, test.two),
                                     predict(prob.three, test.three),
                                     predict(prob.four, test.four),
                                     predict(prob.five, test.five),
                                     predict(prob.six, test.six),
                                     predict(prob.seven, test.seven),
                                     predict(prob.eight, test.eight),
                                     predict(prob.nine, test.nine))
```


```{r}
# Find the index with the highest probability predicted by the models for each class and store it in a list

Label <- rep(NA, nrow(ProbabilityOfEachValue))
for (i in seq(nrow(ProbabilityOfEachValue)))
{
  Label[i] <- which.max(ProbabilityOfEachValue[i,])
}
(conf.log <- table(Label, pca.tst$label))
```


```{r}
(sum(conf.log) - sum(diag(conf.log))) / sum(conf.log)
```

The misclassification rate is 13.15%. 

## KNN 
```{r}
knn.pred <- knn(pca.tr[,-1], pca.tst[, -21], pca.tr[,1], k =5) # use CV the best k is 5 
knn.MSE <- 1 - mean(knn.pred == pca.tst[,21])
table(knn.pred, pca.tst[,21])
knn.MSE

```
Clearly, with KNN method, the misclassifcation rate is 3.02%. 4-9 pair is the hardest one to predict.


## SVM
```{r}
pca.svm <- svm(label~., data = pca.tr, method="C-classification", kernal="radial", gamma= 0.1, cost=10)
svm.pred <- predict(pca.svm, pca.tst)
table(svm.pred, pca.tst[,21])
knn.MSE <- 1- mean(svm.pred == pca.tst[,21])
knn.MSE
```
Clearly, with SVM method, the misclassifcation rate is 2.17%. 4-9 pair is the hardest one to predict.