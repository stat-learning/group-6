---
title: "Exploratory Data Analysis"
output: github_document
---

### Data description

Our data is the MNIST database of handwritten digits, collected from this page http://yann.lecun.com/exdb/mnist/. The digits have been size-normalized and centered in a fixed-size image.

The training set contains 60,000 examples, and the test set 10,000 examples.

Our observations are images. Each image is composed of 28*28 = 748 pixels, where each pixel value ranges from 0-255. We use all 748 pixels as our predictors. $n$ = 70,000; $p$ = 748.

### Data exploration

```{r loading-data}
# This part read idx files and store image data into train$x and 
# test$x in matrix form, store corresponding labels in train$y 
# and test$y in array form 
load_image_file <- function(filename) {
   ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
}

load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
}

train <- load_image_file("data/train-images-idx3-ubyte")
test <- load_image_file("data/t10k-images-idx3-ubyte")

train$y <- load_label_file("data/train-labels-idx1-ubyte")
test$y <- load_label_file("data/t10k-labels-idx1-ubyte")  

```

We can take a look at the first 25 images. 

```{r}
par(mfrow=c(5,5))
par(mar=c(0.1,0.1,0.1,0.1))
for (i in 1:25){
  image(matrix(train$x[i,],28)[,28:1])
}
```

```{r data-preprocessing}
library(ggplot2)

# normalize pixel values from 0-255 to 0-1 
train$st <- train$x/255
test$st <- test$x/255

# for each digit class, calculate the mean digit, mean distance from the mean digit and 
# variance of the distance

label <- seq(0,9) 

distance_mean <- rep(NA,10)
distance_var <- rep(NA,10)
mean_digits <- matrix(rep(0,7840),10)

par(mfrow=c(2,5))

for(i in 1:10){
  df <- train$st[train$y ==label[i],]   # create a dataframe with only the i-th digit
  mean_digit <- colMeans(df)            # get the mean form for each digit(0-9)
  mean_digits[i,] <- mean_digit         # store the mean image into mean_digits 
  dif <- sweep(df,2,mean_digit)         # store the difference for each image from its mean form 
  distance <- rowSums(dif^2)            # distance is an arrary of Euclidean distances from each image to its mean form
  distance_mean[i] <- mean(distance)
  distance_var[i] <- var(distance)
  
  # plot density of distances for each digit 
  plot(density(distance), xlab = "distance", ylab = "density", main = as.character(label[i]))
}
```

```{r}
dist_df <- data.frame(x = distance_mean, y = distance_var)
colnames(dist_df) <- c("mean", "var")
rownames(dist_df) <- label
print(dist_df)
```

From the summary table, we can see that digit 1 has the lowest mean distance; this indicates that most people write it similarly. 2 has the highest mean distance, so people tend to write 2 in different ways. We also see that 6 and 9 have the highest distance variance, which suggests these two digits have the most variation in people's writing styles. 

We can take a look at the mean image for each digit. 

```{r}
# Display the mean form for each digit
par(mfrow=c(2,5))
par(mar=c(0.1,0.1,0.1,0.1))
for (i in 1:10){
  image(matrix(mean_digits[i,],28)[,28:1]*255)
}

p <- prcomp(train$x)
library(tidyverse)
q <- as.data.frame(p$x)
p1 <- ggplot(q, aes(x = PC1, y = PC2)) +geom_point(size = 1)
p1

logistic

library(dplyr)
train_df <- data.frame(train$y, train$x) %>%
  rename(label = train.y)
test_df <- data.frame(test$y, test$x) %>%
  rename(label = test.y)

library(nnet)
lm <- multinom(label ~ ., data = train_df)
summary(lm)
z <- summary(lm)$coefficients/summary(lm)$standard.errors
z
p <- (1 - pnorm(abs(z), 0, 1)) * 2
p

library(class)
trainCl <- factor(0:9)
testCl <- factor(test_df[label, ])
knn(train = train_df$label, test = test_df$label, trainCl, k = 6)
###My knn gives an error that I can't figure out. I think I may be writing the cl wrong which is supposed to be the classifications. It says the length is wrong

library(tree)
t1 <- tree(label ~ ., 
           data = train_df, split = "deviance")
t1
plot(t1)
text(t1, pretty = 0)
set.seed(40)
t1cv <- cv.tree(t1)
t1cv


t1cv$size[which.min(t1cv$dev)]
t1prune <- prune.tree(t1, best = 13)
plot(t1prune)
text(t1prune, pretty = 0)

library(randomForest)
rf1 <- randomForest(label ~ .,            data = train_df, mtry = 4) 
rf2 <- randomForest(label ~ .,            data = train_df, mtry = 1) 

pred1 = predict(rf1, newdata = test_df)
mean((pred1 - test_df$label)^2)

pred2 = predict(rf2, newdata = test_df)
mean((pred2 - test_df$label)^2)

set.seed(1)
 library(gbm)
boost = gbm(label ~., 
            data=train_df, distribution = "gaussian", n.trees=50, interaction.depth = 1, shrinkage = 0.1)
summary(boost)

```




