---
title: "Exploratory Data Analysis"
output: github_document
---

### Data description

Our data is the MNIST database of handwritten digits, collected from this page http://yann.lecun.com/exdb/mnist/. The digits have been size-normalized and centered in a fixed-size image.

The training set contains 60,000 examples, and the test set 10,000 examples.

Our observations are images. Each image is composed of 28*28 = 748 pixels, where each pixel value ranges from 0-255. We use all 748 pixels as our predictors. $n$ = 70,000; $p$ = 748.

### Data exploration

```{r loading-data}
# This part read idx files and store image data into train$x and 
# test$x in matrix form, store corresponding labels in train$y 
# and test$y in array form 
load_image_file <- function(filename) {
   ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
}

load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
}

train <- load_image_file("data/train-images-idx3-ubyte")
test <- load_image_file("data/t10k-images-idx3-ubyte")

train$y <- load_label_file("data/train-labels-idx1-ubyte")
test$y <- load_label_file("data/t10k-labels-idx1-ubyte")  

```

We can take a look at the first 25 images. 

```{r}
par(mfrow=c(5,5))
par(mar=c(0.1,0.1,0.1,0.1))
for (i in 1:25){
  image(matrix(train$x[i,],28)[,28:1])
}
```

```{r data-preprocessing}
library(ggplot2)

# normalize pixel values from 0-255 to 0-1 
train$st <- train$x/255
test$st <- test$x/255

# for each digit class, calculate the mean digit, mean distance from the mean digit and 
# variance of the distance

label <- seq(0,9) 

distance_mean <- rep(NA,10)
distance_var <- rep(NA,10)
mean_digits <- matrix(rep(0,7840),10)

par(mfrow=c(2,5))

for(i in 1:10){
  df <- train$st[train$y ==label[i],]   # create a dataframe with only the i-th digit
  mean_digit <- colMeans(df)            # get the mean form for each digit(0-9)
  mean_digits[i,] <- mean_digit         # store the mean image into mean_digits 
  dif <- sweep(df,2,mean_digit)         # store the difference for each image from its mean form 
  distance <- rowSums(dif^2)            # distance is an arrary of Euclidean distances from each image to its mean form
  distance_mean[i] <- mean(distance)
  distance_var[i] <- var(distance)
  
  # plot density of distances for each digit 
  plot(density(distance), xlab = "distance", ylab = "density", main = as.character(label[i]))
}
```

```{r}
dist_df <- data.frame(x = distance_mean, y = distance_var)
colnames(dist_df) <- c("mean", "var")
rownames(dist_df) <- label
print(dist_df)
```

From the summary table, we can see that digit 1 has the lowest mean distance; this indicates that most people write it similarly. 2 has the highest mean distance, so people tend to write 2 in different ways. We also see that 6 and 9 have the highest distance variance, which suggests these two digits have the most variation in people's writing styles. 

We can take a look at the mean image for each digit. 

```{r}
# Display the mean form for each digit
par(mfrow=c(2,5))
par(mar=c(0.1,0.1,0.1,0.1))
for (i in 1:10){
  image(matrix(mean_digits[i,],28)[,28:1]*255)
}

library(dplyr)
train_df <- data.frame(train$y, train$x) %>%
  rename(label = train.y)
test_df <- data.frame(test$y, test$x) %>%
  rename(label = test.y)

trainp <- sample(1:nrow(train_df), nrow(train_df) * .17)
  train = train_df[trainp,]
testp <- sample(1:nrow(test_df), nrow(test_df) * .2)
  test = test_df[testp,]


```
###Classification Tree
```{r cache=TRUE}
library(tree)
t1 <- tree(as.factor(label) ~ ., 
           data = train, split = "deviance")
t1
plot(t1)
text(t1, pretty = 0)

predt1 = predict(t1, newdata = train, type = "class")
mean((predt1!=train$label))

conf_t1 <- table(predt1, train$label)
conf_t1

set.seed(40)
t1cv <- cv.tree(t1)
t1cv
plot(t1cv$size, t1cv$dev, type = "b", xlab = "n leaves", ylab = "error")

t1cv$size[which.min(t1cv$dev)]
t1prune <- prune.tree(t1, best = 13)
t1prune
plot(t1prune)
text(t1prune, pretty = 0)

predt1p = predict(t1prune, newdata = train, type = "class")
mean((predt1p!=train$label))

conf_t1p <- table(predt1p, train$label)
conf_t1p
```
### Random Forest
```{r cache=TRUE}
library(randomForest)

rf <- randomForest(as.factor(label) ~ .,            data = train, mtry = 262, importance = TRUE) 

plot(rf)

pred.rf <- predict(rf, test, type = "class")
conf.rf <- table(pred.rf, test$label)

(sum(conf.rf) - sum(diag(conf.rf))) / sum(conf.rf)


varImpPlot(rf)

```
### Bagging
```{r cache=TRUE}

rfb <- randomForest(as.factor(label) ~ .,            data = train, mtry = 784, importance = TRUE) 

plot(rfb)

pred.rfb <- predict(rfb, test, type = "class")
conf.rfb <- table(pred.rfb, test$label)
conf.rfb

(sum(conf.rfb) - sum(diag(conf.rfb))) / sum(conf.rfb)


varImpPlot(rfb)

```
###Boosted Random Forest
```{r cache=TRUE}
set.seed(1)
 library(gbm)
boost = gbm(as.factor(label) ~., 
            data=train, distribution = "multinomial", n.trees=50, interaction.depth = 1, shrinkage = 0.1)
summary(boost)

pred.boost <- predict(boost, newdata = test, n.trees = 50)
pred.boost <- apply(pred.boost, 1, which.max)


conf_boost <- table(pred.boost, test$label)
conf_boost
(sum(conf_boost) - sum(diag(conf_boost))) / sum(conf_boost)



```




