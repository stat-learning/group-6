---
title: "Exploratory Data Analysis"
output: github_document
---

### Data description

Our data is the MNIST database of handwritten digits, collected from this page http://yann.lecun.com/exdb/mnist/. The digits have been size-normalized and centered in a fixed-size image.

The training set contains 60,000 examples, and the test set 10,000 examples.

Our observations are images. Each image is composed of 28*28 = 748 pixels, where each pixel value ranges from 0-255. We use all 748 pixels as our predictors. $n$ = 70,000; $p$ = 748.

### Data exploration

```{r loading-data}
# This part read idx files and store image data into train$x and 
# test$x in matrix form, store corresponding labels in train$y 
# and test$y in array form 
load_image_file <- function(filename) {
   ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
}

load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
}

train <- load_image_file("data/train-images-idx3-ubyte")
test <- load_image_file("data/t10k-images-idx3-ubyte")

train$y <- load_label_file("data/train-labels-idx1-ubyte")
test$y <- load_label_file("data/t10k-labels-idx1-ubyte")  

```

We can take a look at the first 25 images. 

```{r}
par(mfrow=c(5,5))
par(mar=c(0.1,0.1,0.1,0.1))
for (i in 1:25){
  image(matrix(train$x[i,],28)[,28:1])
}
```

```{r data-preprocessing}
library(ggplot2)

# normalize pixel values from 0-255 to 0-1 
train$st <- train$x/255
test$st <- test$x/255

# for each digit class, calculate the mean digit, mean distance from the mean digit and 
# variance of the distance

label <- seq(0,9) 

distance_mean <- rep(NA,10)
distance_var <- rep(NA,10)
mean_digits <- matrix(rep(0,7840),10)

par(mfrow=c(2,5))

for(i in 1:10){
  df <- train$st[train$y ==label[i],]   # create a dataframe with only the i-th digit
  mean_digit <- colMeans(df)            # get the mean form for each digit(0-9)
  mean_digits[i,] <- mean_digit         # store the mean image into mean_digits 
  dif <- sweep(df,2,mean_digit)         # store the difference for each image from its mean form 
  distance <- rowSums(dif^2)            # distance is an arrary of Euclidean distances from each image to its mean form
  distance_mean[i] <- mean(distance)
  distance_var[i] <- var(distance)
  
  # plot density of distances for each digit 
  plot(density(distance), xlab = "distance", ylab = "density", main = as.character(label[i]))
}
```

```{r}
dist_df <- data.frame(x = distance_mean, y = distance_var)
colnames(dist_df) <- c("mean", "var")
rownames(dist_df) <- label
print(dist_df)
```

From the summary table, we can see that digit 1 has the lowest mean distance; this indicates that most people write it similarly. 2 has the highest mean distance, so people tend to write 2 in different ways. We also see that 6 and 9 have the highest distance variance, which suggests these two digits have the most variation in people's writing styles. 

We can take a look at the mean image for each digit. 

```{r}
# Display the mean form for each digit
par(mfrow=c(2,5))
par(mar=c(0.1,0.1,0.1,0.1))
for (i in 1:10){
  image(matrix(mean_digits[i,],28)[,28:1]*255)
}

library(dplyr)
train_df <- data.frame(train$y, train$x) %>%
  rename(label = train.y)
test_df <- data.frame(test$y, test$x) %>%
  rename(label = test.y)


```
###Classification Tree
```{r}
library(tree)
t1 <- tree(as.factor(label) ~ ., 
           data = train_df, split = "deviance")
t1
plot(t1)
text(t1, pretty = 0)

predt1 = predict(t1, newdata = test_df, type = "class")
mean((predt1!=test_df$label))

set.seed(40)
t1cv <- cv.tree(t1)
t1cv


t1cv$size[which.min(t1cv$dev)]
t1prune <- prune.tree(t1, best = 13)
plot(t1prune)
text(t1prune, pretty = 0)

predt1p = predict(t1prune, newdata = test_df, type = "class")
mean((predt1p!=test_df$label))
```
### Random Forest
```{r}
library(randomForest)
rf1 <- randomForest(as.factor(label) ~ .,            data = train_df, mtry = 4) 
rf2 <- randomForest(as.factor(label) ~ .,            data = train_df, mtry = 1) 

predrf1 = predict(rf1, newdata = test_df, type = "class")
mean((predrf1!=test_df$label))

predrf2 = predict(rf2, newdata = test_df, type = "class")
mean((predrf2!=test_df$label))
```
###Boosted Random Forest
```{r}
set.seed(1)
 library(gbm)
boost = gbm(as.factor(label) ~., 
            data=train_df, distribution = "gaussian", n.trees=50, interaction.depth = 1, shrinkage = 0.1)
summary(boost)

predb = predict(boost, newdata = test_df, type = "response", n.trees=50)
mean((predb!=test_df$label))

```




